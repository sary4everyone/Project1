---
title: "Regression"
author: "Sarthak"
date: "2/1/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/Admin/Desktop/Project1")
#.libPaths('C:/Users/Admin/Documents/R/win-library/4.1')

library(reticulate)
use_python("C:/Users/Admin/Anaconda3/python.exe")
#use_python("C:/Users/Admin/AppData/Local/Programs/Python/Python36/python.exe")
```

## R

```{r}
library(janitor)
library(pacman)
library(tidyverse)
library(tidymodels)
library(listenv)
library(magrittr)
library(lmtest)
library(tseries)
library(nortest)
library(car)
library(sandwich)
library(lattice)
library(viridisLite)
library(leaps)
library(glue)
library(GGally)
```

```{r}
dt4 <- data.table::fread(file = './Data/cps5_small.csv')

split <- dt4 %>% initial_split(0.8)
dt4_train = training(split)
dt4_test = testing(split)

dt4_train %<>% as.data.frame(dt4_train)
dt4_test %<>% as.data.frame(dt4_test)
n_train = nrow(dt4_train)
```

```{r}
all_cols = colnames(dt4_train)
binary_cols = which(dt4_train %>%  apply(.,2,function(x){all(x %in% 0:1)}) == 1) %>%  names()
numeric_cols = setdiff(dt4_train %>% select_if(.,is.numeric) %>%  colnames,binary_cols)
dt4_train %<>% select(all_of(binary_cols),all_of(numeric_cols))
dt4_train %<>% select(everything(),wage)

#######Panel/Pair Plots
{
# panel.hist <- function(x,...){
#     usr <- par("usr")
#     on.exit(par(usr))
#     par(usr = c(usr[1:2], 0, 1.5) )
#     
#     h <- hist(x, plot = FALSE, breaks = 30)
#     breaks <- h$breaks; nB <- length(breaks); y <- h$counts; y <- y/max(y)
#     rect(breaks[-nB], 0, breaks[-1], y, ...)
# }
# panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...){
#     usr <- par("usr")
#     on.exit(par(usr))
#     par(usr = c(0, 1, 0, 1))
#     r <- cor(x, y, use = "complete.obs")
#     txt <- format(c(r, 0.123456789), digits = digits)[1]
#     txt <- paste0(prefix, txt)
#     if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
#     text(0.5, 0.5, txt, cex = 2)
# }
# # panel.box <- function(x, ...){
# #     usr <- par("usr")
# #     on.exit(par(usr))
# #     par(usr = c(0, 1, 0, 1))
# #     boxplot(as.formula(glue('{col2} ~ {col1}',col1=x,col2=y)),data=dt4_train,plot=TRUE)
# # }
# pairs(dt4_train[,numeric_cols], 
#       diag.panel = panel.hist, 
#       lower.panel = panel.smooth, 
#       upper.panel = panel.cor, 
#       col = "dodgerblue4", 
#       pch = 21, 
#       bg = adjustcolor("dodgerblue3", alpha = 0.2)
# )
# # pairs(dt4_train[,c(binary_cols,'wage')],
# #       diag.panel = panel.hist,
# #       lower.panel = panel.box,
# #       upper.panel = panel.cor,
# #       col = "dodgerblue4",
# #       pch = 21,
# #       bg = adjustcolor("dodgerblue3", alpha = 0.2)
# # )
# dt4_train[,numeric_cols] %>% ggscatmat(alpha = 0.2)
# dt4_train %>% ggpairs()
}
##########

cor(dt4_train[,all_cols[-1]]) %>% round(2)
cor(dt4_train[,numeric_cols[-1]]) %>% round(2)

myPanel <- function(x, y, z, ...){
  lattice::panel.levelplot(x,y,z,...)
  my_text <- ifelse(!is.na(z), paste0(round(z, 4)), "")
  lattice::panel.text(x, y, my_text)
}
mask = cor(dt4_train[,numeric_cols], use = "complete.obs")
mask[upper.tri(mask, diag = TRUE)] <- NA
lattice::levelplot(
  mask, 
  panel = myPanel, 
  col.regions = viridisLite::viridis(100), 
  main = 'Correlation of numerical variables'
)
                   
#####                   
                   
for (i in seq_along(binary_cols)){
  #boxplot(as.formula(glue('wage ~ {col}',col=names(dt4_train[, binary_cols])[i])),data=dt4_train)
  g = ggplot(dt4_train,aes(y=wage,x=!!sym(binary_cols[i]) )) +
    geom_boxplot(aes(group=!!sym(binary_cols[i]) ),alpha=0.4)
  print(g)
}

for (i in seq_along(numeric_cols)){
  #hist(dt4_train[,numeric_cols[i]],breaks=30,main=glue('{col}',col=numeric_cols[i]),col = "cornflowerblue",)
  g = ggplot(dt4_train,aes(x=!!sym(numeric_cols[i]) )) +
    geom_histogram(col = "black",fill = "cornflowerblue", alpha=0.8) 
  print(g)
}

for (i in 1:(length(binary_cols)-1)){
  for (j in (i+1):(length(binary_cols))){
    CGPfunctions::PlotXTabs(dt4_train,i,j,"stacked") #percent, side
  }
}

for (i in 1:(length(numeric_cols)-1)){
  for (j in (i+1):(length(numeric_cols))){
    car::scatterplot(as.formula(glue('{col2} ~ {col1}',col1=numeric_cols[i],col2=numeric_cols[j])),data=dt4_train)
  }
}
#par(mar=c(1,1,1,1))
```

```{r}
#dt4_train %<>%  mutate(educ_bins=cut(educ,breaks=2))
#dt4_train$educ_bins %<>%  as.integer
#dt4_train %<>% select(educ_bins, everything())
#dt4_train %<>% select(-educ)

mdl0_fit <- lm(formula = "log(wage) ~ 1 + educ + I(educ^2) + exper + I(exper^2) + metro + south + west + midwest + female + black", data = dt4_train)
print(summary(mdl0_fit))

mdl1_fit <- lm(formula = "log(wage) ~ 1 + educ + I(educ^2) + exper + I(exper^2) + metro + female", data = dt4_train)
print(summary(mdl1_fit))

plot(mdl1_fit$fitted.values, mdl1_fit$residuals, type = "p", pch = 21, bg = "cornflowerblue", main = "Residuals vs Fitted", ylab = "residuals", xlab = "fitted values", cex = 1.5)

hist(mdl1_fit$residuals, col = "cornflowerblue", breaks = 30, main = "Residual Histogram")

qqnorm(mdl1_fit$residuals, main = "Q-Q plot of residuals", pch = 21, bg = "cornflowerblue", cex = 1.5); qqline(mdl1_fit$residuals, col = "red", lwd = 2)

forecast::ggtsdisplay(mdl1_fit$residuals,lag.max=30)
```

```{r}
#Homoscedasticity Tests
homoscedasticity_tests <- function(errors){
  data.frame(
    'test_name' = c('Breusch Pagan', 'White', 'Goldfeld-Quandt'),
    'p_value' = c(
      lmtest::bptest(mdl1_fit)$p.value,
      lmtest::bptest(mdl1_fit, ~ metro*south*midwest*female*educ*exper + I(educ^2) + I(exper^2), data = dt4_train)$p.value,
      lmtest::gqtest(mdl1_fit)$p.value
    )
  )}
homoscedasticity_tests(mdl1_fit$residuals)
```

```{r}
#Autocorrelation Tests
autocorrelation_tests <- function(errors){
  data.frame(
    'test_name' = c('Durbin Watson', 'Breusch Godfrey'),
    'p_value' = c(
      lmtest::dwtest(mdl1_fit, alternative = "two.sided")$p.value %>%  round(4),
      lmtest::bgtest(mdl1_fit, order = 2)$p.value %>%  round(4)
    )
  )}
autocorrelation_tests(mdl1_fit$residuals)
```

```{r}
#Normality Tests
normality_tests <- function(errors){
  data.frame(
    'test_name' = c('Anderson Darling', 'Wilk Shapiro', 'Kolmogorov-Smirnov', 'Cramer Von Misses', 'Jarque Bera'),
    'p_value' = c(
      nortest::ad.test(errors)$p.value %>%  round(4),
      shapiro.test(errors)$p.value %>%  round(4),
      ks.test(errors, y = "pnorm", alternative = "two.sided")$p.value %>%  round(4),
      nortest::cvm.test(errors)$p.value %>%  round(4),
      tseries::jarque.bera.test(errors)$p.value %>%  round(4)
    )
  )}
normality_tests(mdl1_fit$residuals)

```

```{r}
#Goodness of Fit Tests
gof_tests <- function(mdl_fit){
  data.frame(
    'test_name' = c('AIC', 'BIC', 'R2', 'Adj. R2', 'F-Test p-val'),
    'value' = c(
      AIC(mdl_fit) %>%  round(2),
      BIC(mdl_fit) %>%  round(2),
      summary(mdl_fit)$r.squared %>%  round(4),
      summary(mdl_fit)$adj.r.squared%>%  round(4),
      1-pf(summary(mdl_fit)$fstatistic[1],df1 =summary(mdl_fit)$fstatistic[2], df2=summary(mdl_fit)$fstatistic[3])
    )
  )}
gof_tests(mdl1_fit)
```

```{r}
#Multicollinearity Tests
multicol_tests <- function(mdl_fit){
  data.frame(
    'test_name' = c('VIF'),
    'col_name' = car::vif(mdl_fit) %>%  names,
    'value' = car::vif(mdl_fit) %>%  as.numeric %>%  round(2)
  )}
multicol_tests(mdl1_fit)

mdl_sml_fit = lm(formula = "log(wage) ~ educ + exper + metro + female", data = dt4_train)
multicol_tests(mdl_sml_fit)
```

```{r}
#Linearity Tests
linearity_tests <- function(mdl_fit,dt){
  data.frame(
    'test_name' = c('Reset','Rainbow'), #'Harvey Collier'
    'p_value' = c(
      #lmtest::harvtest(formula(mdl_fit), data = dt)$p.value %>%  round(4),
      lmtest::resettest(formula(mdl_fit), data = dt, power = 3, type = "fitted")$p.value %>%  round(4),
      lmtest::raintest(formula(mdl_fit), data = dt, order.by = ~educ)$p.value %>%  round(4)
    )
)}
linearity_tests(mdl1_fit,dt4_train)
```

```{r}
#Outlier and Influencial Observations Tests
olsrr::ols_plot_resid_stud(mdl1_fit)
olsrr::ols_plot_resid_lev(mdl1_fit)
olsrr::ols_plot_cooksd_bar(mdl1_fit)
olsrr::ols_plot_cooksd_chart(mdl1_fit)
olsrr::ols_plot_dffits(mdl1_fit)
olsrr::ols_plot_dfbetas(mdl1_fit)

for (i in seq_along(numeric_cols)){
  #boxplot(as.formula(glue('wage ~ {col}',col=names(dt4_train[, binary_cols])[i])),data=dt4_train)
  g = ggplot(dt4_train,aes(x=!!sym(numeric_cols[i]) )) +
    geom_boxplot(alpha=0.4)
  print(g)
}
```

## Python

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import seaborn as sns

import statsmodels.api as sm
import statsmodels.formula.api as smf
import statsmodels.stats.diagnostic as sm_diagnostic
import statsmodels.stats.stattools as sm_tools
import statsmodels.stats as smstats
import statsmodels.stats.outliers_influence as oi
#oi.summary_table: from statsmodels.stats.outliers_influence import summary_table
from statsmodels.sandbox.regression.predstd import wls_prediction_std
from statsmodels.compat import lzip

from sklearn.model_selection import train_test_split

from matplotlib.lines import Line2D
```

```{python}

dt4 = pd.read_csv('./Data/cps5_small.csv')
print(dt4.dtypes)
print(dt4.info())
print(dt4.describe())

```

```{python}
X = dt4[['black', 'educ', 'exper', 'faminc', 'female', 'metro', 'midwest','south', 'west']]
y = dt4['wage']
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=123)

dt4_train=pd.concat([X_train,y_train],axis=1)
all_cols = dt4_train.columns
binary_cols = [col for col in dt4_train if np.isin(dt4_train[col].unique(),[0,1]).all()]
numeric_cols = list(set(all_cols)-set(binary_cols))

####
axes = pd.plotting.scatter_matrix(
  dt4_train[numeric_cols], alpha=0.2, marker="o", hist_kwds=dict(edgecolor="black", bins=30), edgecolor="black",figsize = (20, 15))
  corr = dt4_train[numeric_cols].corr().values
for i, j in zip(*plt.np.triu_indices_from(axes, k = 1)): #triu - TRI-angle U-pper
    _ = axes[i,j].set_xlim((1.1, 1.12))
    _ = axes[i,j].set_ylim((1.1, 1.12))
    _ = axes[i,j].annotate("%.3f" %corr[i,j], (0.5, 0.5), xycoords='axes fraction',ha='center',va='center', fontsize = 10)
_ = plt.tight_layout()
plt.show()

####

_ = pd.plotting.scatter_matrix(
  dt4_train[numeric_cols], 
  alpha = 0.2, marker = "o",hist_kwds = dict(edgecolor = "black", linewidth = 1, bins = 30), edgecolor = "black")
_ = plt.tight_layout()
plt.show()

####

print(dt4_train[all_cols].corr())
print(dt4_train[numeric_cols].corr())

corr_mat = dt4_train[numeric_cols].corr()
mask = np.zeros_like(corr_mat, dtype = np.bool)
mask[np.triu_indices_from(mask)] = True
cmap = sns.diverging_palette(220, 10, as_cmap = True)

_ = plt.figure(figsize = (10, 8))
_ = plt.title('Correlation of numerical variables', size = 15)
_ = sns.heatmap(corr_mat, mask = mask, cmap = cmap, annot = True,fmt = '.2f')
#_ = plt.ylim((len(corr_mat), 0))
plt.show()

for col in binary_cols:
  _ = plt.figure()
  _ = sns.boxplot(y='wage',x=col,data=dt4_train)
  #_ = sns.swarmplot(y='wage',x=col,data=dt4_train,color=".25")
  _ = plt.tight_layout()
  plt.show()

for col in numeric_cols:
  _ = plt.figure()
  _ = sns.histplot(x=col,data=dt4_train)
  _ = plt.tight_layout()
  plt.show()

for i in range(0,len(binary_cols)-1):
  for j in range(i+1,len(binary_cols)):
    _ = plt.figure()
    _ = sns.countplot(x=binary_cols[i],hue=binary_cols[j],data=dt4_train)
    _ = plt.tight_layout()
    plt.show()

for i in range(0,len(numeric_cols)-1):
  for j in range(i+1,len(numeric_cols)):
    _ = plt.figure()
    _ = sns.scatterplot(x=numeric_cols[i],y=numeric_cols[j],data=dt4_train)
    _ = plt.tight_layout()
    plt.show()

```

```{python}
mdl0 = smf.ols("np.log(wage) ~ educ + I(educ**2) + exper + I(exper**2) + metro + south + west + midwest + female + black", data = dt4_train)
mdl0_fit = mdl0.fit()
print(mdl0_fit.summary())

mdl1 = smf.ols("np.log(wage) ~ educ + I(educ**2) + exper + I(exper**2) + metro + south + female", data = dt4_train)
mdl1_fit = mdl1.fit()
print(mdl1_fit.summary())

_ = plt.figure()
_ = plt.plot(mdl1_fit.fittedvalues, mdl1_fit.resid, linestyle = "None", marker = "o", markeredgecolor = "black")
_ = plt.tight_layout()
plt.show()

_ = plt.figure()
_ = plt.hist(mdl1_fit.resid, bins = 30, edgecolor = "black")
_ = plt.tight_layout()
plt.show()

_ = plt.figure()
_ = stats.probplot(mdl1_fit.resid, dist = "norm", plot=plt)
_ = plt.tight_layout()
plt.show()

_ = plt.figure()
_ = plt.plot(list(mdl1_fit.resid.reset_index().index), mdl1_fit.resid, '-o')
plt.show()

_ = plt.figure()
_ = plot_acf(mdl1_fit.resid)
plt.show()

_ = plt.figure()
_ = plot_pacf(mdl1_fit.resid,method='ols')
plt.show()
```
```{python}
#Homoscedasticity
def homoscedasticity_tests(mdl_fit,mdl):
  tests = ['Breusch-Pagan', 'Goldfeld-Quandt', 'White']
  name = ['LM-statistic', 'LM-p-value', 'F-statistic', 'F-p-value']
  bp_test = sm_diagnostic.het_breuschpagan(resid = mdl_fit.resid, exog_het = pd.DataFrame(mdl.exog, columns = mdl.exog_names))
  gq_test = sm_diagnostic.het_goldfeldquandt(y = mdl_fit.model.endog, x = mdl_fit.model.exog, alternative = "two-sided")
  w_test = sm_diagnostic.het_white(resid = mdl_fit.model.endog, exog = mdl_fit.model.exog)
  return(pd.DataFrame(
    [np.round(bp_test, 4),
     np.round([-1,-1]+list(gq_test[0:2]), 4),
     np.round(bp_test, 4)], columns=name,index=tests))
     
homoscedasticity_tests(mdl1_fit,mdl1)
```

```{python}
#Autocorrelation Tests
def autocorrelation_tests(mdl_fit):
  tests = ['Durbin-Watson', 'Breusch-Godfrey']
  name = ['LM statistic', 'LM-p-value', 'F-statistic', 'F-p-value']
  sm_test = sm_tools.durbin_watson(mdl_fit.resid)
  bg_test = sm_diagnostic.acorr_breusch_godfrey(mdl_fit, nlags = 2)
  return(pd.DataFrame(
    [np.round([-1,-1]+[sm_test]+[-1], 4),
     np.round(bg_test, 4)], columns=name,index=tests))
     
autocorrelation_tests(mdl1_fit)
```

```{python}
#Normality Tests
def normality_tests(mdl_fit):
  tests =["Anderson-Darling", "Shapiro-Wilk", "Kolmogorov-Smirnov", "Jarque-Bera"]
  name = ['statistic','p-value']
  ad_test = sm_diagnostic.normal_ad(x = mdl_fit.resid)
  sw_test = stats.shapiro(x = mdl_fit.resid)
  ks_test = sm_diagnostic.kstest_normal(x = mdl_fit.resid, dist = "norm")
  #cvm_test = stats.cramervonmises(data = mdl_fit.resid, dist = 'norm')
  jb_test = sm_tools.jarque_bera(mdl_fit.resid)[0:2]
  return(pd.DataFrame(
    [np.round(ad_test, 4),
     np.round(sw_test[:], 4),
     np.round(ks_test, 4),
     np.round(jb_test, 4)], columns=name,index=tests))
     
normality_tests(mdl1_fit)
```


```{python}
#Multicollinearity Tests
def multicol_tests(mdl):
  name = mdl.exog_names[1:]
  vif_out = [smstats.outliers_influence.variance_inflation_factor(mdl.exog, i) for i in range(1, mdl.exog.shape[1]) ]
  df_vif = pd.DataFrame(index=name)
  df_vif['VIF'] = vif_out
  return(df_vif)

multicol_tests(mdl1)

mdl_sml = smf.ols(formula = "np.log(wage) ~ educ + exper + metro + female", data = dt4_train)
mdl_sml_fit = mdl_sml.fit()
multicol_tests(mdl_sml)
```

```{python}
#Linearity Tests
def linearity_tests(mdl_fit,mdl,dt,col):
  tests =["Harvey-Collier", "Reset-Ramsey"]
  name = ['statistic','p-value']
  mdl_ord_fit=smf.ols(mdl.formula, data =dt.iloc[np.argsort(dt[col]),:]).fit() 
  hc_test = [i for i in sm_diagnostic.linear_rainbow(mdl_ord_fit)]
  rr_test = [n[0] for n in oi.reset_ramsey(mdl_fit, degree = 2).statistic]+[oi.reset_ramsey(mdl_fit, degree = 2).pvalue.ravel()[0]]
  return(pd.DataFrame(
    [np.round(hc_test, 4),
     np.round(rr_test, 4)], columns=name,index=tests))

linearity_tests(mdl1_fit,mdl1,dt4_train,'educ')
```


```{python}
def gof_tests(mdl_fit):
  formulas =['AIC', 'BIC', 'R2', 'Adj-R2', 'F-Test-p-val']
  name = ['value']
  return(pd.DataFrame(
    [np.round(mdl_fit.aic, 2),
     np.round(mdl_fit.bic, 2),
     np.round(mdl_fit.rsquared, 2),
     np.round(mdl_fit.rsquared_adj, 2),
     np.round(mdl_fit.f_pvalue, 2)], columns=name,index=formulas))
  
gof_tests(mdl1_fit)
```


```{python}
#Outlier and Influencial Observations Tests
mdl1_cooksd = mdl1_fit.get_influence().cooks_distance[0]
critical_cd = 4/dt4_train.shape[0]

# _ = plt.figure()
# _ = plt.plot(x=[i for i in range(0,dt4_train.shape[0])],y=mdl1_cooksd)
# plt.show()

cooksd_index = mdl1_cooksd > critical_cd
pd.concat([dt4_train.iloc[cooksd_index,].reset_index(),pd.Series(mdl1_cooksd[cooksd_index],name='cooks_dist')],axis=1)
```

```{r}
#LAST2
```

```{r}
#LAST
```

